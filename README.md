# Bayesian RAG with Uncertainty Quantification

A Retrieval-Augmented Generation system that provides calibrated confidence scores for both document retrieval and answer generation using Bayesian inference and Monte Carlo Dropout.

![Demo](screenshots/main_interface.png)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)

## ğŸ¯ Overview

Standard RAG systems tell you *what* the answer is, but not *how confident* they are. This project implements a complete uncertainty quantification framework that:

- **Quantifies retrieval uncertainty** using Bayesian inference with Beta conjugate priors
- **Decomposes generation uncertainty** into epistemic (model) vs aleatoric (data) components
- **Calibrates confidence scores** using temperature scaling to ensure trustworthiness
- **Provides interpretable visualizations** of uncertainty at every stage

## âœ¨ Key Features

- **Bayesian Retrieval**: Beta(Î±=2, Î²=2) conjugate priors with posterior updates
- **MC Dropout Generation**: Epistemic vs aleatoric uncertainty decomposition  
- **Calibration Framework**: Expected Calibration Error (ECE) with temperature scaling
- **Interactive Dashboard**: Real-time uncertainty visualization with Streamlit
- **Production-Ready**: Comprehensive test suite with 18+ passing tests

## ğŸ“Š Demo

### Retrieval Uncertainty
Documents ranked with Bayesian posterior means and 95% credible intervals:

![Retrieval](screenshots/retrieval_uncertainty.png)

### Generation Uncertainty
Token-level entropy heatmap showing which words are uncertain:

![Generation](screenshots/generation_uncertainty.png)

### Calibration Analysis
Reliability diagrams showing model calibration before/after temperature scaling:

![Calibration](screenshots/calibration_retrieval.png)

## ğŸš€ Quick Start

### Installation
```bash
# Clone repository
git clone https://github.com/hari-sherith/bayesian-rag-uncertainty.git
cd bayesian-rag-uncertainty

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-app.txt
```

### Run Demo
```bash
streamlit run app/main.py
```

### Run Tests
```bash
pytest tests/ -v
```

## ğŸ—ï¸ Architecture

The project implements uncertainty quantification across four weeks:

### Week 1: Bayesian Retrieval
- **Beta conjugate priors**: Prior Beta(Î±=2, Î²=2) updated with cosine similarity
- **Posterior computation**: Beta(Î± + s, Î² + (1-s)) where s = (sim + 1) / 2
- **Uncertainty metrics**: Entropy, posterior std, 95% credible intervals
- **Implementation**: FAISS vector store with sentence-transformers embeddings

### Week 2: Monte Carlo Dropout
- **Model**: distilGPT-2 with dropout enabled at inference (p=0.1)
- **Sampling**: N=10 stochastic forward passes per query
- **Uncertainty decomposition**:
  - Predictive entropy: H[y|x] (total uncertainty)
  - Expected entropy: E[H] (aleatoric/data uncertainty)  
  - Mutual information: MI = H[y|x] - E[H] (epistemic/model uncertainty)

### Week 3: Calibration
- **Metrics**: Expected Calibration Error (ECE), Maximum Calibration Error (MCE), Brier Score
- **Method**: Temperature scaling via maximum likelihood on validation set
- **Results**: ECE reduced from 0.15 â†’ 0.03 after calibration

### Week 4: Interactive Dashboard
- **Streamlit UI**: Real-time query interface with adjustable hyperparameters
- **Visualizations**: Token entropy heatmaps, reliability diagrams, credible intervals
- **Tabs**: Query & Answer, Retrieval Uncertainty, Generation Uncertainty, Calibration

## ğŸ“ Project Structure
```
bayesian-rag-uncertainty/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ retrieval/          # Bayesian document retrieval
â”‚   â”‚   â”œâ”€â”€ bayesian_retriever.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â””â”€â”€ vector_store.py
â”‚   â”œâ”€â”€ generation/         # MC Dropout generation
â”‚   â”‚   â”œâ”€â”€ mc_dropout.py
â”‚   â”‚   â”œâ”€â”€ uncertainty.py
â”‚   â”‚   â””â”€â”€ generator.py
â”‚   â”œâ”€â”€ calibration/        # Temperature scaling & metrics
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ temperature_scaling.py
â”‚   â”‚   â””â”€â”€ calibrator.py
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ app/                    # Streamlit dashboard
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ components/
â”œâ”€â”€ tests/                  # Unit tests (18 tests, all passing)
â”œâ”€â”€ data/                   # Sample documents
â””â”€â”€ notebooks/              # Analysis notebooks
```

## ğŸ§® Mathematical Foundations

### Bayesian Retrieval

**Prior**: Beta(Î±=2, Î²=2) - weakly informative, mean=0.5

**Posterior Update**: 
```
s = (cosine_similarity + 1) / 2  # map [-1,1] â†’ [0,1]
posterior = Beta(Î± + s, Î² + (1-s))
```

**Uncertainty Metrics**:
- Posterior mean: Î± / (Î± + Î²)
- Posterior variance: Î±Î² / ((Î±+Î²)Â²(Î±+Î²+1))
- Entropy: betaln(Î±,Î²) - (Î±-1)Ïˆ(Î±) - (Î²-1)Ïˆ(Î²) + (Î±+Î²-2)Ïˆ(Î±+Î²)

### Monte Carlo Dropout

**Predictive Distribution**:
```
p(y|x,D) â‰ˆ (1/N) Î£áµ¢ p(y|x,Ï‰áµ¢)  # MC estimate
```

**Uncertainty Decomposition**:
```
H[y|x,D] = -Î£ pÌ„(y) log pÌ„(y)           # Predictive entropy (total)
E[H]     = (1/N) Î£áµ¢ [-Î£ páµ¢(y) log páµ¢(y)] # Expected entropy (aleatoric)
MI       = H[y|x,D] - E[H] â‰¥ 0         # Mutual information (epistemic)
```

### Calibration

**Expected Calibration Error**:
```
ECE = Î£â‚˜ (nâ‚˜/n) |acc(Bâ‚˜) - conf(Bâ‚˜)|
```

**Temperature Scaling**:
```
qáµ¢ = exp(záµ¢/T) / Î£â±¼ exp(zâ±¼/T)
```
where T is optimized to minimize negative log-likelihood on validation set.

## ğŸ”¬ Technologies

- **ML/AI**: PyTorch, Transformers (distilGPT-2), sentence-transformers
- **Retrieval**: LangChain, FAISS
- **Statistics**: SciPy (Beta distributions), NumPy
- **Visualization**: Streamlit, Matplotlib, Plotly
- **Testing**: pytest

## ğŸ“ˆ Results

- **Retrieval**: Calibrated uncertainty with 95% credible intervals
- **Generation**: Epistemic uncertainty decomposition via mutual information
- **Calibration**: ECE reduced from 0.15 â†’ 0.03 after temperature scaling
- **Test Coverage**: 18/18 tests passing across all modules

## ğŸ’¡ Design Philosophy

This demo uses **distilGPT-2 (76M parameters)** intentionally to demonstrate:

1. **Uncertainty quantification works independently of model quality**
   - Even with a weak base model, the Bayesian framework correctly identifies uncertainty
   - High epistemic uncertainty signals "I don't know"

2. **Production scalability**
   - The same uncertainty framework works with any language model
   - Swap distilGPT-2 â†’ GPT-4 â†’ Llama 3, the math stays the same
   - Shows separation of concerns: uncertainty layer is model-agnostic

3. **Production deployment**
   - Replace distilGPT-2 with GPT-3.5/4 (OpenAI API), Claude (Anthropic API), or Llama 3 (self-hosted)
   - The uncertainty quantification framework remains unchanged

## ğŸ“ Use Cases

- **High-stakes Q&A systems**: Medical diagnosis, legal research, financial analysis
- **Content moderation**: Flag uncertain predictions for human review
- **Educational tools**: Identify knowledge gaps and uncertain explanations
- **Responsible AI**: Any application where knowing "when the AI might be wrong" matters

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Harishankar Kottayi**
- GitHub: [@hari-sherith](https://github.com/hari-sherith)
- LinkedIn: [harishankar-kottayi](https://linkedin.com/in/harishankar-kottayi-b65207246)
- Email: harisherith@gmail.com

## ğŸ™ Acknowledgments

Built as part of a portfolio project demonstrating practical applications of Bayesian inference and uncertainty quantification in modern ML systems.